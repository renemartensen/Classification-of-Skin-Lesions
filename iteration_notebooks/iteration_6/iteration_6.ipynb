{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 6.1\n",
    "\n",
    "Reduce LR on Plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_6_1\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "\n",
    "# Now import HomemadeDataloader from homemade_dataloader.py\n",
    "from homemade_dataloader import DataloaderFactory\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = [1171,1171,1171,1171,1171,1171,1171]\n",
    "print(sum(dist))\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AUC scores for each class (from the PR curve)\n",
    "auc_scores = [0.51, 0.96, 0.81, 0.55, 0.64, 0.66, 0.92]  # Replace with your actual AUC values\n",
    "\n",
    "# Calculate weights inversely proportional to AUC scores\n",
    "class_weights = {i: 1 / score for i, score in enumerate(auc_scores)}\n",
    "\n",
    "# Normalize the weights to keep them reasonably scaled\n",
    "max_weight = max(class_weights.values())\n",
    "class_weights = {k: v / max_weight for k, v in class_weights.items()}\n",
    "\n",
    "weights = class_weights\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from custom_model_iteration_6 import CustomModel\n",
    "\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile(optimizer=\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, class_weight=weights, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=50, checkpoint_path=model_dir, lr=[1e-4, 1e-3])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 6.2\n",
    "\n",
    "Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_6_2\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "\n",
    "# Now import HomemadeDataloader from homemade_dataloader.py\n",
    "from homemade_dataloader import DataloaderFactory\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = [1171,1171,1171,1171,1171,1171,1171]\n",
    "print(sum(dist))\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AUC scores for each class (from the PR curve)\n",
    "auc_scores = [0.51, 0.96, 0.81, 0.55, 0.64, 0.66, 0.92]  # Replace with your actual AUC values\n",
    "\n",
    "# Calculate weights inversely proportional to AUC scores\n",
    "class_weights = {i: 1 / score for i, score in enumerate(auc_scores)}\n",
    "\n",
    "# Normalize the weights to keep them reasonably scaled\n",
    "max_weight = max(class_weights.values())\n",
    "class_weights = {k: v / max_weight for k, v in class_weights.items()}\n",
    "\n",
    "weights = class_weights\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from custom_model_iteration_6 import CustomModel\n",
    "\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile(optimizer=\"ADAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, class_weight=weights, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=50, checkpoint_path=model_dir, lr=[1e-4, 1e-3])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_recall_curve, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "class CombinedHistory:\n",
    "    def __init__(self, combined_history_dict):\n",
    "        self.history = combined_history_dict\n",
    "\n",
    "def combine_histories(histories):\n",
    "    combined_history = {}\n",
    "    for key in histories[0].history.keys():\n",
    "        combined_history[key] = []\n",
    "        for history in histories:\n",
    "            combined_history[key].extend(history.history[key])\n",
    "    return CombinedHistory(combined_history)\n",
    "\n",
    "# Define the function to plot all metrics including balanced accuracy\n",
    "def show_all_plots(histories, model, validation_generator):\n",
    "\n",
    "\n",
    "    if isinstance(histories, list):  # Check if it's a list of histories\n",
    "        history = combine_histories(histories)\n",
    "    else:  # If single history is passed, use it directly\n",
    "        history = histories.history\n",
    "    \n",
    "\n",
    "\n",
    "    # Assuming `validation_generator` has class indices mapped to class names\n",
    "    class_names = list(validation_generator.class_names)\n",
    "    print(class_names)\n",
    "\n",
    "    # Calculate balanced accuracy for each epoch\n",
    "\n",
    "    # Get true labels and predictions for the entire validation set\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X_val_batch, y_val_batch = validation_generator[i]\n",
    "        y_pred_batch = model.predict(X_val_batch, verbose=0)\n",
    "        y_pred.extend(y_pred_batch)  # Keep the raw probabilities for precision-recall\n",
    "        y_true.extend(y_val_batch)   # Keep the one-hot encoded true labels\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate balanced accuracy, confusion matrix, and F1 score\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    balanced_acc = balanced_accuracy_score(y_true_labels, y_pred_labels)\n",
    "    conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    print(f\"Accuracy on test set: {model.evaluate(validation_generator, verbose=0)[1]}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true_labels, y_pred_labels, average='weighted')}\")\n",
    "\n",
    "    # Set up a 2x2 grid for the plots\n",
    "    plt.figure(figsize=(16,14), constrained_layout=True)\n",
    "    \n",
    "    # 1. Accuracy plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # 2. Loss plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # 3. Confusion matrix plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # 4. Precision-Recall Curve plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    show_precision_recall(y_true, y_pred)\n",
    "\n",
    "    # Show all the plots\n",
    "    plt.show()\n",
    "\n",
    "def show_precision_recall(y_test, predictions):\n",
    "    class_names = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC']\n",
    "    \n",
    "    # Binarize the output for multiclass\n",
    "    y_test = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    # Compute Precision-Recall and average precision for each class\n",
    "    for i in range(len(class_names)):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], predictions[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], predictions[:, i])\n",
    "\n",
    "    # Compute micro-average Precision-Recall curve and area\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), predictions.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, predictions, average=\"micro\")\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2, label=f'Micro-average (area = {average_precision[\"micro\"]:0.2f})')\n",
    "    for i, color in enumerate(['blue', 'green', 'red', 'purple', 'brown', 'cyan', 'magenta']):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2, label=f'Class {class_names[i]} (area = {average_precision[i]:0.2f})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc='best')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mothermark_cancer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

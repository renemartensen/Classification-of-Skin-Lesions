{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to improve the performance by trying different models and comparing to see if other models can sqeeze more performance out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet169 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, number_of_samples):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load and configure the base model\n",
    "        base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        x = base_model.output\n",
    "        x = BatchNormalization()(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)               \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        preds = Dense(7, activation='softmax')(x)\n",
    "\n",
    "        # Define the complete model\n",
    "        self.model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "        # Set cyclical learning rate parameters\n",
    "        self.lower_bound = 1e-4\n",
    "        self.upper_bound = 1e-3\n",
    "        self.half_cycle_multiple = 4\n",
    "        self.batch_size = 32\n",
    "        self.steps_per_epoch = int(number_of_samples / self.batch_size)\n",
    "        self.half_cycle_length = self.steps_per_epoch * self.half_cycle_multiple\n",
    "        self.full_cycle_length = 2 * self.half_cycle_length\n",
    "        self.learning_rates = []\n",
    "        self.class_weight = None\n",
    "        self.norms = []\n",
    "        self.epochs = 5\n",
    "\n",
    "    def compile(self):\n",
    "        # Compile the internal model\n",
    "        self.model.compile(optimizer=SGD(learning_rate=self.lower_bound, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "\n",
    "\n",
    "    def fit_epochs(self, train_generator, validation_generator, epochs, checkpoint_path, lr=None, class_weight=None):\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        if lr is not None:\n",
    "            self.lower_bound = lr[0]\n",
    "            self.upper_bound = lr[1]\n",
    "        \n",
    "        if class_weight is not None:\n",
    "            self.class_weight = class_weight\n",
    "\n",
    "        class GradientLogger(Callback):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "                self.gradient_norms_log = []\n",
    "                self.norms = []\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    norm = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in self.model.trainable_variables]))\n",
    "                    self.norms.append(norm.numpy())\n",
    "\n",
    "            def get_norms(self):\n",
    "                return self.norms\n",
    "\n",
    "        gradient_logger = GradientLogger(self.model)\n",
    "\n",
    "        \n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=40,          \n",
    "            min_delta=0.001,\n",
    "            start_from_epoch=5\n",
    "        )\n",
    "\n",
    "        # Train the model using the cyclical learning rate scheduler and checkpoint callback\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=1,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[self.lr_scheduler, checkpoint_callback, early_stopping_callback, gradient_logger]\n",
    "        )\n",
    "\n",
    "        self.norms = gradient_logger.get_norms()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def plot_trainable_weights(self):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, self.epochs + 1), self.norms, marker='o', label='Norm of Trainable Variables')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Norm')\n",
    "        plt.title('Norm of Trainable Variables vs Epoch')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "\n",
    "    def unfreeze(self, number_of_layers_to_unfreeze=None):\n",
    "        # Unfreeze all layers in the base model\n",
    "        if number_of_layers_to_unfreeze is None:\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            print(\"All layers have been unfrozen.\")\n",
    "        else:\n",
    "            # Ensure number_of_layers_to_unfreeze is valid\n",
    "            if number_of_layers_to_unfreeze > len(self.model.layers):\n",
    "                print(f\"Warning: Only {len(self.model.layers)} layers exist. Unfreezing all layers.\")\n",
    "                number_of_layers_to_unfreeze = len(self.model.layers)\n",
    "\n",
    "            for layer in self.model.layers[-number_of_layers_to_unfreeze:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "            print(f\"Last {number_of_layers_to_unfreeze} layers have been unfrozen.\")\n",
    "\n",
    "\n",
    "    def lr_find(self, train_generator, validation_generator, min_lr=1e-10, max_lr=0.01, epochs=10, changes_weights=True):\n",
    "\n",
    "        weights = self.model.get_weights()\n",
    "        steps_per_epoch = train_generator.samples / self.batch_size\n",
    "        total_batches = steps_per_epoch * epochs\n",
    "        \n",
    "        # Custom callback for linear learning rate increase\n",
    "        class LinearLRScheduler(Callback):\n",
    "            def __init__(self, min_lr, max_lr, total_batches):\n",
    "                super().__init__()\n",
    "                self.min_lr = min_lr\n",
    "                self.max_lr = max_lr\n",
    "                self.total_batches = total_batches\n",
    "                self.batch_count = 0\n",
    "                self.lr_history = []\n",
    "                self.loss_history = []\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                # Log the loss and learning rate\n",
    "                loss = logs.get('loss')\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "                # Incrementally increase learning rate\n",
    "                self.batch_count += 1\n",
    "                lr = self.min_lr + (self.max_lr - self.min_lr) * (self.batch_count / self.total_batches)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.lr_history.append(lr)\n",
    "\n",
    "        # Instantiate the linear learning rate scheduler callback\n",
    "        lr_scheduler_callback = LinearLRScheduler(min_lr, max_lr, total_batches)\n",
    "        \n",
    "        # Compile the model with the minimum learning rate\n",
    "        self.model.compile(optimizer=SGD(learning_rate=min_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model with the learning rate scheduler to find optimal lr\n",
    "        self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[lr_scheduler_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        if not changes_weights:\n",
    "            self.model.set_weights(weights)\n",
    "\n",
    "        # Plot learning rate vs. loss\n",
    "        plt.plot(lr_scheduler_callback.lr_history, lr_scheduler_callback.loss_history)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Learning Rate')\n",
    "        plt.show()\n",
    "\n",
    "    @property\n",
    "    def lr_scheduler(self):\n",
    "        # Custom cyclical learning rate scheduler\n",
    "        class BatchLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "            def __init__(self, lower_bound, upper_bound, full_cycle_length, learning_rates):\n",
    "                super().__init__()\n",
    "                self.lower_bound = lower_bound\n",
    "                self.upper_bound = upper_bound\n",
    "                self.full_cycle_length = full_cycle_length\n",
    "                self.batch_count = 0\n",
    "                self.learning_rates = learning_rates\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                cycle_position = np.abs((self.batch_count % self.full_cycle_length) / self.full_cycle_length - 0.5) * 2\n",
    "                lr = self.lower_bound + (self.upper_bound - self.lower_bound) * (1 - cycle_position)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.learning_rates.append(lr)\n",
    "                self.batch_count += 1\n",
    "\n",
    "        return BatchLearningRateScheduler(self.lower_bound, self.upper_bound, self.full_cycle_length, self.learning_rates)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model = CustomModel(number_of_samples=1000)\n",
    "# model.compile()\n",
    "# model.fit_epochs(epochs=10, lr=0.001)\n",
    "# model.unfreeze()\n",
    "# lr_history = model.lr_find()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_8_1\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8202 images belonging to 7 classes (dist says 21056)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.densenet import preprocess_input\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "# Now import HomemadeDataloader from homemade_dataloader.py\n",
    "from homemade_dataloader import DataloaderFactory\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = [3008,3008,3008,3008,3008,3008,3008]\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 4.4064 - accuracy: 0.3114\n",
      "Epoch 1: val_loss improved from inf to 36.97437, saving model to ../../models/best_model_iteration_8_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiangodske/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 4651s 7s/step - loss: 4.4064 - accuracy: 0.3114 - val_loss: 36.9744 - val_accuracy: 0.1059\n",
      "Epoch 2/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 3.5854 - accuracy: 0.4809\n",
      "Epoch 2: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 800s 1s/step - loss: 3.5854 - accuracy: 0.4809 - val_loss: 39.2594 - val_accuracy: 0.0787\n",
      "Epoch 3/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 3.1152 - accuracy: 0.5398\n",
      "Epoch 3: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 956s 1s/step - loss: 3.1152 - accuracy: 0.5398 - val_loss: 37.6766 - val_accuracy: 0.0648\n",
      "Epoch 4/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 2.6751 - accuracy: 0.5750\n",
      "Epoch 4: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 1167s 2s/step - loss: 2.6751 - accuracy: 0.5750 - val_loss: 38.5177 - val_accuracy: 0.0457\n",
      "Epoch 5/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 2.2900 - accuracy: 0.5996\n",
      "Epoch 5: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 980s 1s/step - loss: 2.2900 - accuracy: 0.5996 - val_loss: 40.2501 - val_accuracy: 0.0214\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have been unfrozen.\n",
      "Epoch 1/5\n",
      "658/658 [==============================] - 5177s 8s/step - loss: 2.2357 - accuracy: 0.5690 - val_loss: 11.4935 - val_accuracy: 0.0972\n",
      "Epoch 2/5\n",
      "658/658 [==============================] - 38369s 58s/step - loss: 1.8055 - accuracy: 0.7120 - val_loss: 12.3289 - val_accuracy: 0.0885\n",
      "Epoch 3/5\n",
      " 64/658 [=>............................] - ETA: 1:20:01 - loss: 1.6368 - accuracy: 0.7563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 190\u001b[0m, in \u001b[0;36mCustomModel.lr_find\u001b[0;34m(self, train_generator, validation_generator, min_lr, max_lr, epochs, changes_weights)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mmin_lr), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Train the model with the learning rate scheduler to find optimal lr\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changes_weights:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_weights(weights)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=40, checkpoint_path=model_dir, lr=[1e-3, 3e-2])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from plot_utils import show_all_plots\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from itertools import chain\n",
    "\n",
    "class Dataloader(Sequence):\n",
    "    def __init__(self, data, batch_size, image_size, is_validation=False, preprocess_function=None, class_distribuition=[], class_names=[]):\n",
    "        #self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.isValidation = is_validation\n",
    "        self.class_distribution = class_distribuition if not is_validation else []\n",
    "        self.preprocess_function = preprocess_function\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            shear_range=15, \n",
    "            zoom_range=0.4,\n",
    "            rotation_range=45, \n",
    "            brightness_range= [0.9, 1.5],\n",
    "            fill_mode='nearest'\n",
    "            )\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.class_to_label = {class_name: i for i, class_name in enumerate(self.class_names)} # mapping {'class1': 0, 'class2': 1, ...})\n",
    "\n",
    "        self.image_paths, self.labels = zip(*data)\n",
    "        self.image_paths = list(self.image_paths)  # Convert from tuple to list\n",
    "        self.labels = list(self.labels)           # Convert from tuple to list\n",
    "\n",
    "        self.indexes_for_class = [[] for _ in range(self.num_classes)]\n",
    "\n",
    "        self.all_image_indices = self._create_indices_distr() if not self.isValidation else list(range(len(self.image_paths)))\n",
    "        self.samples = len(self.all_image_indices)\n",
    "        #self._shuffle_indices()\n",
    "        print(f\"Found {len(self.image_paths)} images belonging to {self.num_classes} classes (dist says {sum(self.class_distribution)})\")\n",
    "        self._shuffle_indices()\n",
    "\n",
    "\n",
    "    def _create_indices_distr(self):\n",
    "        for index, label in enumerate(self.labels):\n",
    "            self.indexes_for_class[label].append(index) # I assume its list 4 has all the indexes for label class_5\n",
    "\n",
    "        res = list(chain.from_iterable(self.indexes_for_class)) # flattens the list-of-lists to one list\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                image_paths.append(image_path)\n",
    "                labels.append(self.class_to_label[class_name])\n",
    "\n",
    "        #print(f\"Found {len(image_paths)} images belonging to {self.num_classes} classes (dist says {sum(self.class_distribution)})\")\n",
    "        return image_paths, labels\n",
    "\n",
    "    def _preprocess_image(self, image_path):\n",
    "        img = load_img(image_path, target_size=self.image_size)\n",
    "        x = img_to_array(img)\n",
    "\n",
    "        # if not self.isValidation:\n",
    "        #     x = x.reshape((1,) + x.shape)\n",
    "        #     x = next(self.datagen.flow(x, batch_size=1))[0]\n",
    "\n",
    "        if self.preprocess_function:\n",
    "            x = self.preprocess_function(x)\n",
    "        return x\n",
    "\n",
    "    def _shuffle_indices(self):\n",
    "        random.shuffle(self.all_image_indices)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = len(self.all_image_indices) # no_images=100 batchsize=20, then 100/20=5 number of iterations to get through the whole dataset\n",
    "        return num_samples // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        batch_image_indices = self.all_image_indices[start:end]\n",
    "\n",
    "        for image_index in batch_image_indices:\n",
    "            image_path = self.image_paths[image_index]\n",
    "            image = self._preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            labels.append(to_categorical(self.labels[image_index], num_classes=self.num_classes))\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if not self.isValidation:\n",
    "            self._shuffle_indices()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataloaderFactory():\n",
    "    def __init__(self, dir, image_size, batch_size, set_distribution, class_distribution=[], preprocess_function=None):\n",
    "        self.dir = dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.set_distribution = set_distribution\n",
    "        self.class_distribution = class_distribution\n",
    "        self.preprocess_function = preprocess_function\n",
    "        self.class_names = []\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load images and labels as tuples (path, label).\"\"\"\n",
    "        data = []\n",
    "        self.class_names = sorted(os.listdir(self.dir))\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.dir, class_name)\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                data.append((image_path, label))  # Append (path, label) as a tuple\n",
    "        return data\n",
    "\n",
    "    def _split_data(self, data):\n",
    "        \"\"\"Split data into train, validation, and test sets based on the specified distribution.\"\"\"\n",
    "        indices = list(range(len(data)))\n",
    "        _, val_ratio, test_ratio = [p / 100 for p in self.set_distribution]\n",
    "\n",
    "        labels = [label for _, label in data]\n",
    "        train_idxs, test_idxs, _, _ = train_test_split(\n",
    "            indices, labels, test_size=(val_ratio + test_ratio), stratify=labels\n",
    "        )\n",
    "        val_split = val_ratio / (val_ratio + test_ratio)\n",
    "        val_idxs, test_idxs = train_test_split(\n",
    "            test_idxs, test_size=(1 - val_split), stratify=[labels[i] for i in test_idxs]\n",
    "        )\n",
    "\n",
    "        return {\"train\": train_idxs, \"val\": val_idxs, \"test\": test_idxs}\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        \"\"\"Generate dataloaders for training, validation, and testing.\"\"\"\n",
    "        data = self._load_data()  # List of (path, label) tuples\n",
    "        data_splits = self._split_data(data)  # Split data by indices\n",
    "        loaders = {}\n",
    "\n",
    "        for split, indexes in data_splits.items():\n",
    "            preprocess_func = self.preprocess_function if split == \"train\" else None\n",
    "            split_data = [data[i] for i in indexes]  # Subset the data using indices\n",
    "            loaders[split] = Dataloader(\n",
    "                data=split_data,  # Pass the list of tuples\n",
    "                batch_size=self.batch_size,\n",
    "                image_size=self.image_size,\n",
    "                is_validation=(split != \"train\"),\n",
    "                preprocess_function=preprocess_func,\n",
    "                class_distribuition=self.class_distribution,\n",
    "                class_names=self.class_names\n",
    "            )\n",
    "\n",
    "        return loaders[\"train\"], loaders[\"val\"], loaders[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, number_of_samples):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load and configure the base model\n",
    "        base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        x = base_model.output\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Flatten()(x)\n",
    "        preds = Dense(7, activation='softmax')(x)\n",
    "\n",
    "        # Define the complete model\n",
    "        self.model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "        # Set cyclical learning rate parameters\n",
    "        self.lower_bound = 1e-4\n",
    "        self.upper_bound = 1e-3\n",
    "        self.half_cycle_multiple = 6\n",
    "        self.batch_size = 32\n",
    "        self.steps_per_epoch = int(number_of_samples / self.batch_size)\n",
    "        self.half_cycle_length = self.steps_per_epoch * self.half_cycle_multiple\n",
    "        self.full_cycle_length = 2 * self.half_cycle_length\n",
    "        self.learning_rates = []\n",
    "        self.class_weight = None\n",
    "        self.norms = []\n",
    "        self.epochs = 5\n",
    "\n",
    "    def compile(self):\n",
    "        # Compile the internal model\n",
    "        self.model.compile(optimizer=SGD(learning_rate=self.lower_bound), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "\n",
    "\n",
    "    def fit_epochs(self, train_generator, validation_generator, epochs, checkpoint_path, lr=None, class_weight=None):\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        if lr is not None:\n",
    "            self.lower_bound = lr[0]\n",
    "            self.upper_bound = lr[1]\n",
    "        \n",
    "        if class_weight is not None:\n",
    "            self.class_weight = class_weight\n",
    "\n",
    "        class GradientLogger(Callback):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "                self.gradient_norms_log = []\n",
    "                self.norms = []\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    norm = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in self.model.trainable_variables]))\n",
    "                    self.norms.append(norm.numpy())\n",
    "\n",
    "            def get_norms(self):\n",
    "                return self.norms\n",
    "\n",
    "        gradient_logger = GradientLogger(self.model)\n",
    "\n",
    "        \n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=40,          \n",
    "            min_delta=0.001,\n",
    "            start_from_epoch=5\n",
    "        )\n",
    "\n",
    "        # Train the model using the cyclical learning rate scheduler and checkpoint callback\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=1,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[self.lr_scheduler, checkpoint_callback, early_stopping_callback, gradient_logger]\n",
    "        )\n",
    "\n",
    "        self.norms = gradient_logger.get_norms()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def plot_trainable_weights(self):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, self.epochs + 1), self.norms, marker='o', label='Norm of Trainable Variables')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Norm')\n",
    "        plt.title('Norm of Trainable Variables vs Epoch')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers in the base model\n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = True\n",
    "        print(\"All layers have been unfrozen.\")\n",
    "\n",
    "    def lr_find(self, train_generator, validation_generator, min_lr=1e-10, max_lr=0.01, epochs=10):\n",
    "        steps_per_epoch = train_generator.samples / self.batch_size\n",
    "        total_batches = steps_per_epoch * epochs\n",
    "        \n",
    "        # Custom callback for linear learning rate increase\n",
    "        class LinearLRScheduler(Callback):\n",
    "            def __init__(self, min_lr, max_lr, total_batches):\n",
    "                super().__init__()\n",
    "                self.min_lr = min_lr\n",
    "                self.max_lr = max_lr\n",
    "                self.total_batches = total_batches\n",
    "                self.batch_count = 0\n",
    "                self.lr_history = []\n",
    "                self.loss_history = []\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                # Log the loss and learning rate\n",
    "                loss = logs.get('loss')\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "                # Incrementally increase learning rate\n",
    "                self.batch_count += 1\n",
    "                lr = self.min_lr + (self.max_lr - self.min_lr) * (self.batch_count / self.total_batches)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.lr_history.append(lr)\n",
    "\n",
    "        # Instantiate the linear learning rate scheduler callback\n",
    "        lr_scheduler_callback = LinearLRScheduler(min_lr, max_lr, total_batches)\n",
    "        \n",
    "        # Compile the model with the minimum learning rate\n",
    "        self.model.compile(optimizer=SGD(learning_rate=min_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model with the learning rate scheduler to find optimal lr\n",
    "        self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[lr_scheduler_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Plot learning rate vs. loss\n",
    "        plt.plot(lr_scheduler_callback.lr_history, lr_scheduler_callback.loss_history)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Learning Rate')\n",
    "        plt.show()\n",
    "\n",
    "    @property\n",
    "    def lr_scheduler(self):\n",
    "        # Custom cyclical learning rate scheduler\n",
    "        class BatchLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "            def __init__(self, lower_bound, upper_bound, full_cycle_length, learning_rates):\n",
    "                super().__init__()\n",
    "                self.lower_bound = lower_bound\n",
    "                self.upper_bound = upper_bound\n",
    "                self.full_cycle_length = full_cycle_length\n",
    "                self.batch_count = 0\n",
    "                self.learning_rates = learning_rates\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                cycle_position = np.abs((self.batch_count % self.full_cycle_length) / self.full_cycle_length - 0.5) * 2\n",
    "                lr = self.lower_bound + (self.upper_bound - self.lower_bound) * (1 - cycle_position)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.learning_rates.append(lr)\n",
    "                self.batch_count += 1\n",
    "\n",
    "        return BatchLearningRateScheduler(self.lower_bound, self.upper_bound, self.full_cycle_length, self.learning_rates)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model = CustomModel(number_of_samples=1000)\n",
    "# model.compile()\n",
    "# model.fit_epochs(epochs=10, lr=0.001)\n",
    "# model.unfreeze()\n",
    "# lr_history = model.lr_find()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_8_2\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Found 8202 images belonging to 7 classes (dist says 0)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = []\n",
    "print(sum(dist))\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 0.04008614377765143, 3: 0.13472376761969085, 2: 0.08570947862186559, 5: 0.38308410444903407, 0: 0.03958191555403317, 6: 0.31024416909604874, 1: 0.0065704208816761995}\n",
      "{4: 1.301832835044846, 3: 4.375273044997815, 2: 2.78349082823791, 5: 12.440993788819876, 0: 1.2854575792581184, 6: 10.075452716297788, 1: 0.21338020666879728}\n"
     ]
    }
   ],
   "source": [
    "image_counts = {}\n",
    "total_count = 0\n",
    "\n",
    "train_dir = \"../../Data Set Ordered/training data/\"\n",
    "\n",
    "# Loop over each subdirectory in the main directory\n",
    "for subdir in os.listdir(train_dir):\n",
    "    subdir_path = os.path.join(train_dir, subdir)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Count files in the subdirectory (assuming all files are images)\n",
    "        num_images = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "        \n",
    "        # Store the count in the dictionary\n",
    "        image_counts[subdir] = num_images\n",
    "        total_count += num_images\n",
    "\n",
    "\n",
    "#weights = {0: 1.2, 1:0.8,2:1.0, 3:1.2, 4:1.1,5:1.3,6:1.0} \n",
    "weights = {}\n",
    "\n",
    "for subdir, count in image_counts.items():\n",
    "    weight = total_count / (7 * count)\n",
    "    weights.update({int(subdir.split(\"_\")[1]): weight})\n",
    "\n",
    "# Calculate the total weight\n",
    "total_weight = sum(weights.values())\n",
    "\n",
    "# Normalize the weights to make them sum to 1\n",
    "normalized_inverse_weights = {key: value / total_weight for key, value in weights.items()}\n",
    "\n",
    "print(normalized_inverse_weights)\n",
    "\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "256/256 [==============================] - ETA: 0s - loss: 2.0747 - accuracy: 0.3027\n",
      "Epoch 1: val_loss improved from inf to 1.50509, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 36s 133ms/step - loss: 2.0747 - accuracy: 0.3027 - val_loss: 1.5051 - val_accuracy: 0.4624\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiangodske/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - ETA: 0s - loss: 1.2083 - accuracy: 0.5286\n",
      "Epoch 2: val_loss improved from 1.50509 to 1.31428, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 31s 119ms/step - loss: 1.2083 - accuracy: 0.5286 - val_loss: 1.3143 - val_accuracy: 0.5480\n",
      "Epoch 3/5\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.8131 - accuracy: 0.6361\n",
      "Epoch 3: val_loss improved from 1.31428 to 1.12424, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 33s 129ms/step - loss: 0.8131 - accuracy: 0.6361 - val_loss: 1.1242 - val_accuracy: 0.6169\n",
      "Epoch 4/5\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.7186\n",
      "Epoch 4: val_loss improved from 1.12424 to 0.97851, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 30s 117ms/step - loss: 0.5809 - accuracy: 0.7186 - val_loss: 0.9785 - val_accuracy: 0.6562\n",
      "Epoch 5/5\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.7677\n",
      "Epoch 5: val_loss improved from 0.97851 to 0.94089, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 31s 121ms/step - loss: 0.4287 - accuracy: 0.7677 - val_loss: 0.9409 - val_accuracy: 0.6638\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, class_weight=weights, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have been unfrozen.\n",
      "Epoch 1/5\n",
      "256/256 [==============================] - 211s 819ms/step - loss: 0.0980 - accuracy: 0.2445 - val_loss: 1.4498 - val_accuracy: 0.5382\n",
      "Epoch 2/5\n",
      "256/256 [==============================] - 221s 863ms/step - loss: 0.0640 - accuracy: 0.4045 - val_loss: 1.2555 - val_accuracy: 0.6192\n",
      "Epoch 3/5\n",
      "256/256 [==============================] - 114s 443ms/step - loss: 0.0415 - accuracy: 0.5332 - val_loss: 1.2708 - val_accuracy: 0.6209\n",
      "Epoch 4/5\n",
      "256/256 [==============================] - 131s 511ms/step - loss: 0.0281 - accuracy: 0.6145 - val_loss: 1.1993 - val_accuracy: 0.6441\n",
      "Epoch 5/5\n",
      "256/256 [==============================] - 119s 463ms/step - loss: 0.0205 - accuracy: 0.6887 - val_loss: 1.1122 - val_accuracy: 0.6620\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.7460\n",
      "Epoch 1: val_loss improved from inf to 1.14290, saving model to ../../models/best_model_iteration_8_2.h5\n",
      "256/256 [==============================] - 116s 450ms/step - loss: 0.0140 - accuracy: 0.7460 - val_loss: 1.1429 - val_accuracy: 0.6493\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.7434\n",
      "Epoch 2: val_loss did not improve from 1.14290\n",
      "256/256 [==============================] - 194s 758ms/step - loss: 0.0141 - accuracy: 0.7434 - val_loss: 1.1628 - val_accuracy: 0.6505\n",
      "Epoch 3/50\n",
      "122/256 [=============>................] - ETA: 1:15 - loss: 0.0144 - accuracy: 0.7482"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(history2)\n",
      "Cell \u001b[0;32mIn[58], line 103\u001b[0m, in \u001b[0;36mCustomModel.fit_epochs\u001b[0;34m(self, train_generator, validation_generator, epochs, checkpoint_path, lr, class_weight)\u001b[0m\n\u001b[1;32m     95\u001b[0m early_stopping_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     96\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m     97\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,          \n\u001b[1;32m     98\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     99\u001b[0m     start_from_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Train the model using the cyclical learning rate scheduler and checkpoint callback\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_logger\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m gradient_logger\u001b[38;5;241m.\u001b[39mget_norms()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=50, checkpoint_path=model_dir, lr=[1e-5, 1e-4])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from plot_utils import show_all_plots\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

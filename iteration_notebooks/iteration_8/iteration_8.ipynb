{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to improve the performance by trying different models and comparing to see if other models can sqeeze more performance out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet169 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, number_of_samples):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load and configure the base model\n",
    "        base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        x = base_model.output\n",
    "        x = BatchNormalization()(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)               \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        preds = Dense(7, activation='softmax')(x)\n",
    "\n",
    "        # Define the complete model\n",
    "        self.model = Model(inputs=base_model.input, outputs=preds)\n",
    "\n",
    "        # Set cyclical learning rate parameters\n",
    "        self.lower_bound = 1e-4\n",
    "        self.upper_bound = 1e-3\n",
    "        self.half_cycle_multiple = 4\n",
    "        self.batch_size = 32\n",
    "        self.steps_per_epoch = int(number_of_samples / self.batch_size)\n",
    "        self.half_cycle_length = self.steps_per_epoch * self.half_cycle_multiple\n",
    "        self.full_cycle_length = 2 * self.half_cycle_length\n",
    "        self.learning_rates = []\n",
    "        self.class_weight = None\n",
    "        self.norms = []\n",
    "        self.epochs = 5\n",
    "\n",
    "    def compile(self):\n",
    "        # Compile the internal model\n",
    "        self.model.compile(optimizer=SGD(learning_rate=self.lower_bound, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "\n",
    "\n",
    "    def fit_epochs(self, train_generator, validation_generator, epochs, checkpoint_path, lr=None, class_weight=None):\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        if lr is not None:\n",
    "            self.lower_bound = lr[0]\n",
    "            self.upper_bound = lr[1]\n",
    "        \n",
    "        if class_weight is not None:\n",
    "            self.class_weight = class_weight\n",
    "\n",
    "        class GradientLogger(Callback):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "                self.gradient_norms_log = []\n",
    "                self.norms = []\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    norm = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in self.model.trainable_variables]))\n",
    "                    self.norms.append(norm.numpy())\n",
    "\n",
    "            def get_norms(self):\n",
    "                return self.norms\n",
    "\n",
    "        gradient_logger = GradientLogger(self.model)\n",
    "\n",
    "        \n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor='val_loss',  \n",
    "            patience=40,          \n",
    "            min_delta=0.001,\n",
    "            start_from_epoch=5\n",
    "        )\n",
    "\n",
    "        # Train the model using the cyclical learning rate scheduler and checkpoint callback\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=1,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[self.lr_scheduler, checkpoint_callback, early_stopping_callback, gradient_logger]\n",
    "        )\n",
    "\n",
    "        self.norms = gradient_logger.get_norms()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def plot_trainable_weights(self):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, self.epochs + 1), self.norms, marker='o', label='Norm of Trainable Variables')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Norm')\n",
    "        plt.title('Norm of Trainable Variables vs Epoch')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        # Evaluate the model on the validation set\n",
    "        return self.model.evaluate(data)\n",
    "\n",
    "    def unfreeze(self, number_of_layers_to_unfreeze=None):\n",
    "        # Unfreeze all layers in the base model\n",
    "        if number_of_layers_to_unfreeze is None:\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            print(\"All layers have been unfrozen.\")\n",
    "        else:\n",
    "            # Ensure number_of_layers_to_unfreeze is valid\n",
    "            if number_of_layers_to_unfreeze > len(self.model.layers):\n",
    "                print(f\"Warning: Only {len(self.model.layers)} layers exist. Unfreezing all layers.\")\n",
    "                number_of_layers_to_unfreeze = len(self.model.layers)\n",
    "\n",
    "            for layer in self.model.layers[-number_of_layers_to_unfreeze:]:\n",
    "                layer.trainable = True\n",
    "\n",
    "            print(f\"Last {number_of_layers_to_unfreeze} layers have been unfrozen.\")\n",
    "\n",
    "\n",
    "    def lr_find(self, train_generator, validation_generator, min_lr=1e-10, max_lr=0.01, epochs=10, changes_weights=True):\n",
    "\n",
    "        weights = self.model.get_weights()\n",
    "        steps_per_epoch = train_generator.samples / self.batch_size\n",
    "        total_batches = steps_per_epoch * epochs\n",
    "        \n",
    "        # Custom callback for linear learning rate increase\n",
    "        class LinearLRScheduler(Callback):\n",
    "            def __init__(self, min_lr, max_lr, total_batches):\n",
    "                super().__init__()\n",
    "                self.min_lr = min_lr\n",
    "                self.max_lr = max_lr\n",
    "                self.total_batches = total_batches\n",
    "                self.batch_count = 0\n",
    "                self.lr_history = []\n",
    "                self.loss_history = []\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                # Log the loss and learning rate\n",
    "                loss = logs.get('loss')\n",
    "                self.loss_history.append(loss)\n",
    "                \n",
    "                # Incrementally increase learning rate\n",
    "                self.batch_count += 1\n",
    "                lr = self.min_lr + (self.max_lr - self.min_lr) * (self.batch_count / self.total_batches)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.lr_history.append(lr)\n",
    "\n",
    "        # Instantiate the linear learning rate scheduler callback\n",
    "        lr_scheduler_callback = LinearLRScheduler(min_lr, max_lr, total_batches)\n",
    "        \n",
    "        # Compile the model with the minimum learning rate\n",
    "        self.model.compile(optimizer=SGD(learning_rate=min_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model with the learning rate scheduler to find optimal lr\n",
    "        self.model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            class_weight=self.class_weight,\n",
    "            callbacks=[lr_scheduler_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        if not changes_weights:\n",
    "            self.model.set_weights(weights)\n",
    "\n",
    "        # Plot learning rate vs. loss\n",
    "        plt.plot(lr_scheduler_callback.lr_history, lr_scheduler_callback.loss_history)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Learning Rate')\n",
    "        plt.show()\n",
    "\n",
    "    @property\n",
    "    def lr_scheduler(self):\n",
    "        # Custom cyclical learning rate scheduler\n",
    "        class BatchLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "            def __init__(self, lower_bound, upper_bound, full_cycle_length, learning_rates):\n",
    "                super().__init__()\n",
    "                self.lower_bound = lower_bound\n",
    "                self.upper_bound = upper_bound\n",
    "                self.full_cycle_length = full_cycle_length\n",
    "                self.batch_count = 0\n",
    "                self.learning_rates = learning_rates\n",
    "\n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                cycle_position = np.abs((self.batch_count % self.full_cycle_length) / self.full_cycle_length - 0.5) * 2\n",
    "                lr = self.lower_bound + (self.upper_bound - self.lower_bound) * (1 - cycle_position)\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "                self.learning_rates.append(lr)\n",
    "                self.batch_count += 1\n",
    "\n",
    "        return BatchLearningRateScheduler(self.lower_bound, self.upper_bound, self.full_cycle_length, self.learning_rates)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model = CustomModel(number_of_samples=1000)\n",
    "# model.compile()\n",
    "# model.fit_epochs(epochs=10, lr=0.001)\n",
    "# model.unfreeze()\n",
    "# lr_history = model.lr_find()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_8_1\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8202 images belonging to 7 classes (dist says 21056)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n",
      "Found 1758 images belonging to 7 classes (dist says 0)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.densenet import preprocess_input\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "# Now import HomemadeDataloader from homemade_dataloader.py\n",
    "from homemade_dataloader import DataloaderFactory\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = [3008,3008,3008,3008,3008,3008,3008]\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 4.4064 - accuracy: 0.3114\n",
      "Epoch 1: val_loss improved from inf to 36.97437, saving model to ../../models/best_model_iteration_8_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiangodske/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 4651s 7s/step - loss: 4.4064 - accuracy: 0.3114 - val_loss: 36.9744 - val_accuracy: 0.1059\n",
      "Epoch 2/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 3.5854 - accuracy: 0.4809\n",
      "Epoch 2: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 800s 1s/step - loss: 3.5854 - accuracy: 0.4809 - val_loss: 39.2594 - val_accuracy: 0.0787\n",
      "Epoch 3/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 3.1152 - accuracy: 0.5398\n",
      "Epoch 3: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 956s 1s/step - loss: 3.1152 - accuracy: 0.5398 - val_loss: 37.6766 - val_accuracy: 0.0648\n",
      "Epoch 4/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 2.6751 - accuracy: 0.5750\n",
      "Epoch 4: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 1167s 2s/step - loss: 2.6751 - accuracy: 0.5750 - val_loss: 38.5177 - val_accuracy: 0.0457\n",
      "Epoch 5/5\n",
      "658/658 [==============================] - ETA: 0s - loss: 2.2900 - accuracy: 0.5996\n",
      "Epoch 5: val_loss did not improve from 36.97437\n",
      "658/658 [==============================] - 980s 1s/step - loss: 2.2900 - accuracy: 0.5996 - val_loss: 40.2501 - val_accuracy: 0.0214\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have been unfrozen.\n",
      "Epoch 1/5\n",
      "658/658 [==============================] - 5177s 8s/step - loss: 2.2357 - accuracy: 0.5690 - val_loss: 11.4935 - val_accuracy: 0.0972\n",
      "Epoch 2/5\n",
      "658/658 [==============================] - 38369s 58s/step - loss: 1.8055 - accuracy: 0.7120 - val_loss: 12.3289 - val_accuracy: 0.0885\n",
      "Epoch 3/5\n",
      " 64/658 [=>............................] - ETA: 1:20:01 - loss: 1.6368 - accuracy: 0.7563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 190\u001b[0m, in \u001b[0;36mCustomModel.lr_find\u001b[0;34m(self, train_generator, validation_generator, min_lr, max_lr, epochs, changes_weights)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mmin_lr), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Train the model with the learning rate scheduler to find optimal lr\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changes_weights:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_weights(weights)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=40, checkpoint_path=model_dir, lr=[1e-3, 3e-2])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from plot_utils import show_all_plots\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiangodske/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from itertools import chain\n",
    "\n",
    "class Dataloader(Sequence):\n",
    "    def __init__(self, data, batch_size, image_size, is_validation=False, preprocess_function=None, class_distribuition=[], class_names=[]):\n",
    "        #self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.isValidation = is_validation\n",
    "        self.class_distribution = class_distribuition if not is_validation else []\n",
    "        self.preprocess_function = preprocess_function\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            shear_range=50, \n",
    "            height_shift_range=0.2,\n",
    "            width_shift_range=0.2,\n",
    "            rotation_range=360, \n",
    "            brightness_range=[0.3, 1.8], \n",
    "            channel_shift_range= random.uniform(20, 50),\n",
    "            zoom_range=0.3)\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.class_to_label = {class_name: i for i, class_name in enumerate(self.class_names)} # mapping {'class1': 0, 'class2': 1, ...})\n",
    "\n",
    "        self.image_paths, self.labels = zip(*data)\n",
    "        self.image_paths = list(self.image_paths)  # Convert from tuple to list\n",
    "        self.labels = list(self.labels)           # Convert from tuple to list\n",
    "\n",
    "        self.indexes_for_class = [[] for _ in range(self.num_classes)]\n",
    "\n",
    "        self.all_image_indices = self._create_indices_distr() if not self.isValidation else list(range(len(self.image_paths)))\n",
    "        self.samples = len(self.all_image_indices)\n",
    "        #self._shuffle_indices()\n",
    "        print(f\"Found {len(self.image_paths)} images belonging to {self.num_classes} classes (dist says {sum(self.class_distribution)})\")\n",
    "        self._shuffle_indices()\n",
    "\n",
    "\n",
    "    def _create_indices_distr(self):\n",
    "        for index, label in enumerate(self.labels):\n",
    "            self.indexes_for_class[label].append(index) # I assume its list 4 has all the indexes for label class_5\n",
    "\n",
    "        res = list(chain.from_iterable(self.indexes_for_class)) # flattens the list-of-lists to one list\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                image_paths.append(image_path)\n",
    "                labels.append(self.class_to_label[class_name])\n",
    "\n",
    "        #print(f\"Found {len(image_paths)} images belonging to {self.num_classes} classes (dist says {sum(self.class_distribution)})\")\n",
    "        return image_paths, labels\n",
    "\n",
    "    def _preprocess_image(self, image_path):\n",
    "        img = load_img(image_path, target_size=self.image_size)\n",
    "        x = img_to_array(img)\n",
    "\n",
    "        if not self.isValidation:\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            x = next(self.datagen.flow(x, batch_size=1))[0]\n",
    "\n",
    "        if self.preprocess_function:\n",
    "            x = self.preprocess_function(x)\n",
    "        return x\n",
    "\n",
    "    def _shuffle_indices(self):\n",
    "        random.shuffle(self.all_image_indices)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = len(self.all_image_indices) # no_images=100 batchsize=20, then 100/20=5 number of iterations to get through the whole dataset\n",
    "        return num_samples // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        batch_image_indices = self.all_image_indices[start:end]\n",
    "\n",
    "        for image_index in batch_image_indices:\n",
    "            image_path = self.image_paths[image_index]\n",
    "            image = self._preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            labels.append(to_categorical(self.labels[image_index], num_classes=self.num_classes))\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if not self.isValidation:\n",
    "            self._shuffle_indices()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataloaderFactory():\n",
    "    def __init__(self, dir, image_size, batch_size, set_distribution, class_distribution=[], preprocess_function=None):\n",
    "        self.dir = dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.set_distribution = set_distribution\n",
    "        self.class_distribution = class_distribution\n",
    "        self.preprocess_function = preprocess_function\n",
    "        self.class_names = []\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load images and labels as tuples (path, label).\"\"\"\n",
    "        data = []\n",
    "        self.class_names = sorted(os.listdir(self.dir))\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.dir, class_name)\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                data.append((image_path, label))  # Append (path, label) as a tuple\n",
    "        return data\n",
    "\n",
    "    def _split_data(self, data):\n",
    "        \"\"\"Split data into train, validation, and test sets based on the specified distribution.\"\"\"\n",
    "        indices = list(range(len(data)))\n",
    "        _, val_ratio, test_ratio = [p / 100 for p in self.set_distribution]\n",
    "\n",
    "        labels = [label for _, label in data]\n",
    "        train_idxs, test_idxs, _, _ = train_test_split(\n",
    "            indices, labels, test_size=(val_ratio + test_ratio), stratify=labels\n",
    "        )\n",
    "        val_split = val_ratio / (val_ratio + test_ratio)\n",
    "        val_idxs, test_idxs = train_test_split(\n",
    "            test_idxs, test_size=(1 - val_split), stratify=[labels[i] for i in test_idxs]\n",
    "        )\n",
    "\n",
    "        return {\"train\": train_idxs, \"val\": val_idxs, \"test\": test_idxs}\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        \"\"\"Generate dataloaders for training, validation, and testing.\"\"\"\n",
    "        data = self._load_data()  # List of (path, label) tuples\n",
    "        data_splits = self._split_data(data)  # Split data by indices\n",
    "        loaders = {}\n",
    "\n",
    "        for split, indexes in data_splits.items():\n",
    "            preprocess_func = self.preprocess_function if split == \"train\" else None\n",
    "            split_data = [data[i] for i in indexes]  # Subset the data using indices\n",
    "            loaders[split] = Dataloader(\n",
    "                data=split_data,  # Pass the list of tuples\n",
    "                batch_size=self.batch_size,\n",
    "                image_size=self.image_size,\n",
    "                is_validation=(split != \"train\"),\n",
    "                preprocess_function=preprocess_func,\n",
    "                class_distribuition=self.class_distribution,\n",
    "                class_names=self.class_names\n",
    "            )\n",
    "\n",
    "        return loaders[\"train\"], loaders[\"val\"], loaders[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Merge Set Ordered\"\n",
    "iteration = \"iteration_8_2\"\n",
    "model_dir = f'../../models/best_model_{iteration}.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "dist = []\n",
    "print(sum(dist))\n",
    "histories = []\n",
    "\n",
    "factory = DataloaderFactory(data_dir, batch_size=batch_size, image_size=(224,224), set_distribution=(70,15,15), class_distribution=dist, preprocess_function=preprocess_input)\n",
    "\n",
    "train_generator, validation_generator, test_generator = factory.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from custom_model import CustomModel\n",
    "model = CustomModel(number_of_samples=train_generator.samples)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 1.301832835044846, 3: 4.375273044997815, 2: 2.78349082823791, 5: 12.440993788819876, 0: 1.2854575792581184, 6: 10.075452716297788, 1: 0.21338020666879728}\n"
     ]
    }
   ],
   "source": [
    "image_counts = {}\n",
    "total_count = 0\n",
    "\n",
    "train_dir = \"../../Data Set Ordered/training data/\"\n",
    "\n",
    "# Loop over each subdirectory in the main directory\n",
    "for subdir in os.listdir(train_dir):\n",
    "    subdir_path = os.path.join(train_dir, subdir)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Count files in the subdirectory (assuming all files are images)\n",
    "        num_images = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "        \n",
    "        # Store the count in the dictionary\n",
    "        image_counts[subdir] = num_images\n",
    "        total_count += num_images\n",
    "\n",
    "\n",
    "#weights = {0: 1.2, 1:0.8,2:1.0, 3:1.2, 4:1.1,5:1.3,6:1.0} \n",
    "weights = {}\n",
    "\n",
    "for subdir, count in image_counts.items():\n",
    "    weight = total_count / (7 * count)\n",
    "    weights.update({int(subdir.split(\"_\")[1]): weight})\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit_epochs(train_generator, validation_generator, epochs=5, class_weight=weights, checkpoint_path=model_dir)\n",
    "histories.append(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "model.lr_find(train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_epochs(train_generator, validation_generator, epochs=50, checkpoint_path=model_dir, lr=[1e-4, 1e-3])\n",
    "histories.append(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from plot_utils import show_all_plots\n",
    "\n",
    "m = load_model(model_dir)\n",
    "show_all_plots(histories, m, test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
